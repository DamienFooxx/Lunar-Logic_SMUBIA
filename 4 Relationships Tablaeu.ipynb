{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "class CrimeNetworkAnalyzer:\n",
    "    def __init__(self, relationships_df):\n",
    "        self.relationships_df = relationships_df\n",
    "        self.G = nx.Graph()\n",
    "        self.entity_metrics = {}\n",
    "        self.crime_patterns = defaultdict(list)\n",
    "        \n",
    "    def build_network(self):\n",
    "        \"\"\"Build network from relationships data\"\"\"\n",
    "        # Add nodes with attributes\n",
    "        for _, row in self.relationships_df.iterrows():\n",
    "            # Add nodes if they don't exist\n",
    "            if not self.G.has_node(row['subject']):\n",
    "                self.G.add_node(row['subject'], \n",
    "                              type=row['subject_type'],\n",
    "                              crimes=set())\n",
    "            if not self.G.has_node(row['object']):\n",
    "                self.G.add_node(row['object'], \n",
    "                              type=row['object_type'],\n",
    "                              crimes=set())\n",
    "                \n",
    "            # Add edge with attributes\n",
    "            self.G.add_edge(row['subject'], row['object'],\n",
    "                          relationship=row['predicate'],\n",
    "                          crime_type=row['crime_type'],\n",
    "                          evidence_strength=row['evidence_strength'])\n",
    "            \n",
    "            # Update crime types for both nodes\n",
    "            self.G.nodes[row['subject']]['crimes'].add(row['crime_type'])\n",
    "            self.G.nodes[row['object']]['crimes'].add(row['crime_type'])\n",
    "\n",
    "    def calculate_network_metrics(self):\n",
    "        \"\"\"Calculate various network metrics for each entity\"\"\"\n",
    "        # Basic centrality measures\n",
    "        degree_cent = nx.degree_centrality(self.G)\n",
    "        betweenness_cent = nx.betweenness_centrality(self.G)\n",
    "        eigenvector_cent = nx.eigenvector_centrality(self.G, max_iter=1000)\n",
    "        \n",
    "        # Calculate metrics for each node\n",
    "        for node in self.G.nodes():\n",
    "            neighbors = list(self.G.neighbors(node))\n",
    "            crimes = self.G.nodes[node]['crimes']\n",
    "            \n",
    "            self.entity_metrics[node] = {\n",
    "                'type': self.G.nodes[node]['type'],\n",
    "                'degree': self.G.degree(node),\n",
    "                'degree_centrality': degree_cent[node],\n",
    "                'betweenness_centrality': betweenness_cent[node],\n",
    "                'eigenvector_centrality': eigenvector_cent[node],\n",
    "                'num_connections': len(neighbors),\n",
    "                'crimes': list(crimes),\n",
    "                'num_crimes': len(crimes),\n",
    "                'connected_entities': neighbors\n",
    "            }\n",
    "\n",
    "    def identify_crime_patterns(self):\n",
    "        \"\"\"Identify patterns in criminal activities\"\"\"\n",
    "        for node, metrics in self.entity_metrics.items():\n",
    "            # Group by crime type\n",
    "            for crime in metrics['crimes']:\n",
    "                if crime != 'Unknown':\n",
    "                    self.crime_patterns[crime].append({\n",
    "                        'entity': node,\n",
    "                        'type': metrics['type'],\n",
    "                        'centrality': metrics['degree_centrality']\n",
    "                    })\n",
    "\n",
    "    def clean_network_data(self):\n",
    "        \"\"\"Clean network data by handling null values and removing unknown crimes\"\"\"\n",
    "        # Remove edges with unknown crime types\n",
    "        edges_to_remove = [(u, v) for u, v, data in self.G.edges(data=True) \n",
    "                          if data.get('crime_type') == 'Unknown']\n",
    "        self.G.remove_edges_from(edges_to_remove)\n",
    "        \n",
    "        # Remove isolated nodes (nodes with no connections after edge removal)\n",
    "        isolated_nodes = list(nx.isolates(self.G))\n",
    "        self.G.remove_nodes_from(isolated_nodes)\n",
    "        \n",
    "        # Clean remaining node attributes\n",
    "        for node in self.G.nodes():\n",
    "            # Remove 'Unknown' from crime sets\n",
    "            self.G.nodes[node]['crimes'] = {\n",
    "                str(crime) for crime in self.G.nodes[node]['crimes']\n",
    "                if crime != 'Unknown'\n",
    "            }\n",
    "            \n",
    "            # If no crimes left, add placeholder\n",
    "            if not self.G.nodes[node]['crimes']:\n",
    "                self.G.nodes[node]['crimes'] = {'Unspecified'}\n",
    "            \n",
    "            # Ensure type is not null\n",
    "            if not self.G.nodes[node]['type']:\n",
    "                self.G.nodes[node]['type'] = 'Unspecified'\n",
    "\n",
    "        # Clean remaining edge attributes\n",
    "        for u, v, data in self.G.edges(data=True):\n",
    "            if not data.get('evidence_strength'):\n",
    "                data['evidence_strength'] = 'unspecified'\n",
    "        \n",
    "        # Update metrics after cleaning\n",
    "        self.calculate_network_metrics()\n",
    "        self.identify_crime_patterns()\n",
    "\n",
    "    def export_for_tableau(self, output_path_prefix):\n",
    "        \"\"\"Export data in Tableau-friendly format\"\"\"\n",
    "        # Nodes table\n",
    "        nodes_data = []\n",
    "        for node, metrics in self.entity_metrics.items():\n",
    "            nodes_data.append({\n",
    "                'Entity': node,\n",
    "                'Type': metrics['type'],\n",
    "                'Degree': metrics['degree'],\n",
    "                'DegreeCentrality': metrics['degree_centrality'],\n",
    "                'BetweennessCentrality': metrics['betweenness_centrality'],\n",
    "                'EigenvectorCentrality': metrics['eigenvector_centrality'],\n",
    "                'NumConnections': metrics['num_connections'],\n",
    "                'NumCrimes': metrics['num_crimes'],\n",
    "                'Crimes': ';'.join(metrics['crimes'])\n",
    "            })\n",
    "        \n",
    "        # Edges table\n",
    "        edges_data = []\n",
    "        for edge in self.G.edges(data=True):\n",
    "            edges_data.append({\n",
    "                'Source': edge[0],\n",
    "                'Target': edge[1],\n",
    "                'Relationship': edge[2]['relationship'],\n",
    "                'CrimeType': edge[2]['crime_type'],\n",
    "                'EvidenceStrength': edge[2]['evidence_strength']\n",
    "            })\n",
    "        \n",
    "        # Crime patterns table\n",
    "        patterns_data = []\n",
    "        for crime_type, entities in self.crime_patterns.items():\n",
    "            for entity in entities:\n",
    "                patterns_data.append({\n",
    "                    'CrimeType': crime_type,\n",
    "                    'Entity': entity['entity'],\n",
    "                    'EntityType': entity['type'],\n",
    "                    'Centrality': entity['centrality']\n",
    "                })\n",
    "        \n",
    "        # Export to CSV\n",
    "        pd.DataFrame(nodes_data).to_csv(f'{output_path_prefix}_nodes.csv', index=False)\n",
    "        pd.DataFrame(edges_data).to_csv(f'{output_path_prefix}_edges.csv', index=False)\n",
    "        pd.DataFrame(patterns_data).to_csv(f'{output_path_prefix}_patterns.csv', index=False)\n",
    "        \n",
    "        return pd.DataFrame(nodes_data), pd.DataFrame(edges_data), pd.DataFrame(patterns_data)\n",
    "\n",
    "    def get_summary_statistics(self):\n",
    "        \"\"\"Get summary statistics of the network\"\"\"\n",
    "        return {\n",
    "            'num_nodes': self.G.number_of_nodes(),\n",
    "            'num_edges': self.G.number_of_edges(),\n",
    "            'avg_degree': sum(dict(self.G.degree()).values()) / self.G.number_of_nodes(),\n",
    "            'density': nx.density(self.G),\n",
    "            'num_components': nx.number_connected_components(self.G),\n",
    "            'avg_clustering': nx.average_clustering(self.G)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial network size:\n",
      "Nodes: 2152\n",
      "Edges: 5528\n",
      "\n",
      "Cleaning network data...\n",
      "\n",
      "Network size after removing unknown crimes:\n",
      "Nodes: 462 (1690 removed)\n",
      "Edges: 1268 (4260 removed)\n",
      "\n",
      "Network Summary Statistics:\n",
      "num_nodes: 462.0000\n",
      "num_edges: 1268.0000\n",
      "avg_degree: 5.4892\n",
      "density: 0.0119\n",
      "num_components: 69.0000\n",
      "avg_clustering: 0.6422\n",
      "\n",
      "Top 10 Most Central Entities:\n",
      "                           Entity Type  DegreeCentrality  NumCrimes\n",
      "44                             Un  ORG          0.197397          4\n",
      "273                           Itf  ORG          0.084599          4\n",
      "361                          Unon  ORG          0.065076          2\n",
      "271                         Unmik  ORG          0.062907          4\n",
      "279                            Al  PER          0.056399          4\n",
      "275                       Airport  ORG          0.054230          3\n",
      "290  Air Traffic Control Services  ORG          0.054230          1\n",
      "342                            Ed  PER          0.052061          4\n",
      "344                            Wi  PER          0.052061          4\n",
      "433                        Unicef  ORG          0.045553          2\n",
      "\n",
      "Top Crime Patterns:\n",
      "CrimeType\n",
      "CONSPIRACY          516\n",
      "FRAUD               264\n",
      "CORRUPTION          138\n",
      "TRAFFICKING          88\n",
      "MONEY_LAUNDERING     40\n",
      "CYBERCRIME           38\n",
      "Gambling              4\n",
      "Battery               4\n",
      "Theft                 4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the relationships data\n",
    "relationships_df = pd.read_csv('/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/FINAL FINAL PLEASE/Data/process3_crime_relationships_enhanced.csv')\n",
    "\n",
    "# Initialize and run the analyzer\n",
    "analyzer = CrimeNetworkAnalyzer(relationships_df)\n",
    "analyzer.build_network()\n",
    "\n",
    "# Record initial stats\n",
    "initial_nodes = analyzer.G.number_of_nodes()\n",
    "initial_edges = analyzer.G.number_of_edges()\n",
    "\n",
    "print(f\"\\nInitial network size:\")\n",
    "print(f\"Nodes: {initial_nodes}\")\n",
    "print(f\"Edges: {initial_edges}\")\n",
    "\n",
    "# Clean the network data\n",
    "print(\"\\nCleaning network data...\")\n",
    "analyzer.clean_network_data()\n",
    "\n",
    "# Print cleaning impact\n",
    "print(f\"\\nNetwork size after removing unknown crimes:\")\n",
    "print(f\"Nodes: {analyzer.G.number_of_nodes()} ({initial_nodes - analyzer.G.number_of_nodes()} removed)\")\n",
    "print(f\"Edges: {analyzer.G.number_of_edges()} ({initial_edges - analyzer.G.number_of_edges()} removed)\")\n",
    "\n",
    "# Calculate metrics and export\n",
    "analyzer.calculate_network_metrics()\n",
    "analyzer.identify_crime_patterns()\n",
    "nodes_df, edges_df, patterns_df = analyzer.export_for_tableau('/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/FINAL FINAL PLEASE/Data/Tablaeu Data/crime_network_clean')\n",
    "\n",
    "# Export data for Tableau\n",
    "nodes_df, edges_df, patterns_df = analyzer.export_for_tableau('/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/FINAL FINAL PLEASE/Data/Tablaeu Data/crime_network')\n",
    "\n",
    "# Print summary statistics\n",
    "stats = analyzer.get_summary_statistics()\n",
    "print(\"\\nNetwork Summary Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Print top entities by centrality\n",
    "print(\"\\nTop 10 Most Central Entities:\")\n",
    "nodes_df_sorted = nodes_df.sort_values('DegreeCentrality', ascending=False).head(10)\n",
    "print(nodes_df_sorted[['Entity', 'Type', 'DegreeCentrality', 'NumCrimes']])\n",
    "\n",
    "# Print most common crime patterns\n",
    "print(\"\\nTop Crime Patterns:\")\n",
    "crime_counts = patterns_df['CrimeType'].value_counts()\n",
    "print(crime_counts.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freshsmubia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
