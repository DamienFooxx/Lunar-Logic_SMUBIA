{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Set, Tuple\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Entity Analysis:\n",
    "\n",
    "Entity role detection (e.g., director, CEO)\n",
    "Confidence scoring for entities\n",
    "Multiple mention tracking\n",
    "Entity disambiguation\n",
    "\n",
    "\n",
    "Relationship Detection:\n",
    "\n",
    "Entity pair analysis\n",
    "Sentence-level relationship extraction\n",
    "Confidence scoring for relationships\n",
    "Action pattern recognition\n",
    "\n",
    "\n",
    "Enhanced Information Extraction:\n",
    "\n",
    "Context preservation\n",
    "Role extraction for persons and organizations\n",
    "Relationship confidence scoring\n",
    "Detailed mention tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedEntityAnalyzer:\n",
    "    \"\"\"\n",
    "    Advanced entity analysis system specifically designed for processing\n",
    "    criminal investigation documents and extracting detailed entity relationships.\n",
    "    Incorporates both original and processed text for improved accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load the larger spaCy model for better accuracy\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "        \n",
    "        # Initialize matchers and storage\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        self.phrase_matcher = PhraseMatcher(self.nlp.vocab)\n",
    "        self.entity_registry = defaultdict(dict)\n",
    "        \n",
    "        # Initialize patterns for relationship detection\n",
    "        self._initialize_patterns()\n",
    "    \n",
    "    def _initialize_patterns(self):\n",
    "        \"\"\"\n",
    "        Initialize custom patterns for entity and relationship analysis.\n",
    "        Includes role patterns, action patterns, and relationship indicators.\n",
    "        \"\"\"\n",
    "        # Role patterns (positions and titles)\n",
    "        self.role_patterns = [\n",
    "            'director', 'ceo', 'chairman', 'manager', 'officer',\n",
    "            'president', 'head', 'leader', 'founder', 'owner',\n",
    "            'executive', 'administrator', 'supervisor', 'coordinator'\n",
    "        ]\n",
    "        \n",
    "        # Action patterns indicating relationships\n",
    "        self.action_patterns = [\n",
    "            'collaborated', 'worked with', 'partnered', 'associated',\n",
    "            'connected to', 'linked to', 'involved with', 'related to',\n",
    "            'managed', 'supervised', 'reported to', 'directed'\n",
    "        ]\n",
    "        \n",
    "        # Relationship indicators for crime context\n",
    "        self.relationship_indicators = {\n",
    "            'HIERARCHICAL': [\n",
    "                'reports to', 'supervised by', 'managed by', 'works under',\n",
    "                'directed by', 'controlled by', 'overseen by'\n",
    "            ],\n",
    "            'COLLABORATION': [\n",
    "                'worked with', 'partnered with', 'collaborated with',\n",
    "                'assisted', 'supported', 'aided', 'helped'\n",
    "            ],\n",
    "            'CRIMINAL_ASSOCIATION': [\n",
    "                'conspired with', 'colluded with', 'involved in',\n",
    "                'participated in', 'engaged in', 'associated with'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Add patterns to matcher\n",
    "        for pattern_type, patterns in self.relationship_indicators.items():\n",
    "            pattern = [{'LOWER': {'IN': patterns}}]\n",
    "            self.matcher.add(pattern_type, [pattern])\n",
    "    \n",
    "    def process_document(self, original_text: str, processed_text: str, \n",
    "                        existing_entities: Dict[str, Set[str]], metadata: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a single document using both original and processed text.\n",
    "        \n",
    "        Args:\n",
    "            original_text: Raw text content\n",
    "            processed_text: Cleaned text from Step 1\n",
    "            existing_entities: Entities already identified in Step 1\n",
    "            metadata: Document metadata from Step 1\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing enhanced entity information and relationships\n",
    "        \"\"\"\n",
    "        # Process both versions of text\n",
    "        original_doc = self.nlp(original_text)\n",
    "        processed_doc = self.nlp(processed_text)\n",
    "        \n",
    "        # Extract and combine entities from both versions\n",
    "        original_entities = self._extract_entities(original_doc)\n",
    "        processed_entities = self._extract_entities(processed_doc)\n",
    "        \n",
    "        # Compare and combine entity information\n",
    "        enhanced_entities = self._compare_entity_versions(original_entities, \n",
    "                                                        processed_entities,\n",
    "                                                        existing_entities)\n",
    "        \n",
    "        # Extract relationships between entities\n",
    "        relationships = self._extract_relationships(original_doc, enhanced_entities)\n",
    "        \n",
    "        # Enhance relationships with metadata\n",
    "        enhanced_relationships = self._incorporate_metadata(relationships, metadata)\n",
    "        \n",
    "        return {\n",
    "            'entities': enhanced_entities,\n",
    "            'relationships': enhanced_relationships,\n",
    "            'confidence_scores': self._calculate_confidence_scores(enhanced_entities)\n",
    "        }\n",
    "    \n",
    "    def _extract_entities(self, doc: Doc) -> Dict[str, Set[str]]:\n",
    "        \"\"\"\n",
    "        Extract entities from a spaCy Doc object with enhanced categorization.\n",
    "        \"\"\"\n",
    "        entities = defaultdict(set)\n",
    "        \n",
    "        # Enhanced entity label mapping\n",
    "        label_mapping = {\n",
    "            'GPE': 'LOC',\n",
    "            'LOC': 'LOC',\n",
    "            'FAC': 'LOC',\n",
    "            'PERSON': 'PER',\n",
    "            'ORG': 'ORG',\n",
    "            'NORP': 'MISC',\n",
    "            'PRODUCT': 'MISC',\n",
    "            'EVENT': 'MISC',\n",
    "            'LAW': 'MISC',\n",
    "            'LANGUAGE': 'MISC'\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            category = label_mapping.get(ent.label_, 'MISC')\n",
    "            # Clean and normalize entity text\n",
    "            clean_text = self._clean_entity_text(ent.text)\n",
    "            if clean_text:\n",
    "                entities[category].add(clean_text)\n",
    "        \n",
    "        return dict(entities)\n",
    "    \n",
    "    def _clean_entity_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize entity text.\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace and standardize quotes\n",
    "        text = ' '.join(text.split())\n",
    "        text = text.replace('\"', '\"').replace('\"', '\"')\n",
    "        return text.strip()\n",
    "    \n",
    "    def _compare_entity_versions(self, original_entities: Dict, \n",
    "                               processed_entities: Dict,\n",
    "                               existing_entities: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Compare and combine entities from different sources.\n",
    "        Assigns confidence scores based on occurrence in multiple sources.\n",
    "        \"\"\"\n",
    "        combined_entities = defaultdict(lambda: {\n",
    "            'entities': set(),\n",
    "            'confidence_scores': defaultdict(float)\n",
    "        })\n",
    "        \n",
    "        # Process each entity type\n",
    "        for entity_type in set(original_entities) | set(processed_entities) | set(existing_entities):\n",
    "            # Get entities from each source\n",
    "            original = original_entities.get(entity_type, set())\n",
    "            processed = processed_entities.get(entity_type, set())\n",
    "            existing = existing_entities.get(entity_type, set())\n",
    "            \n",
    "            # Combine entities and calculate confidence scores\n",
    "            all_entities = original | processed | existing\n",
    "            for entity in all_entities:\n",
    "                confidence = 0.0\n",
    "                if entity in original: confidence += 0.3\n",
    "                if entity in processed: confidence += 0.3\n",
    "                if entity in existing: confidence += 0.4\n",
    "                \n",
    "                combined_entities[entity_type]['entities'].add(entity)\n",
    "                combined_entities[entity_type]['confidence_scores'][entity] = min(1.0, confidence)\n",
    "        \n",
    "        return dict(combined_entities)\n",
    "    \n",
    "    def _extract_relationships(self, doc: Doc, entities: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract relationships between entities with enhanced context.\n",
    "        \"\"\"\n",
    "        relationships = []\n",
    "        entity_pairs = self._generate_entity_pairs(entities)\n",
    "        \n",
    "        for entity1, entity2, entity_types in entity_pairs:\n",
    "            relationship = self._analyze_relationship(doc, entity1, entity2, entity_types)\n",
    "            if relationship:\n",
    "                relationships.append(relationship)\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "    def _generate_entity_pairs(self, entities: Dict) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Generate meaningful entity pairs for relationship analysis.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        # Focus on PER-ORG, PER-PER, and ORG-ORG relationships\n",
    "        priority_types = ['PER', 'ORG']\n",
    "        \n",
    "        for type1 in priority_types:\n",
    "            for type2 in priority_types:\n",
    "                if type1 in entities and type2 in entities:\n",
    "                    entities1 = entities[type1]['entities']\n",
    "                    entities2 = entities[type2]['entities']\n",
    "                    \n",
    "                    if type1 == type2:\n",
    "                        # Avoid self-pairs for same type\n",
    "                        pairs.extend([\n",
    "                            (e1, e2, (type1, type2))\n",
    "                            for i, e1 in enumerate(entities1)\n",
    "                            for e2 in list(entities1)[i+1:]\n",
    "                        ])\n",
    "                    else:\n",
    "                        # Cross-type pairs\n",
    "                        pairs.extend([\n",
    "                            (e1, e2, (type1, type2))\n",
    "                            for e1 in entities1\n",
    "                            for e2 in entities2\n",
    "                        ])\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _analyze_relationship(self, doc: Doc, entity1: str, \n",
    "                            entity2: str, entity_types: Tuple) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze the relationship between two entities with enhanced context.\n",
    "        \"\"\"\n",
    "        # Find sentences containing both entities\n",
    "        relevant_sents = []\n",
    "        for sent in doc.sents:\n",
    "            if entity1.lower() in sent.text.lower() and entity2.lower() in sent.text.lower():\n",
    "                relevant_sents.append(sent)\n",
    "        \n",
    "        if not relevant_sents:\n",
    "            return None\n",
    "        \n",
    "        # Analyze the relationship\n",
    "        relationship = {\n",
    "            'entity1': {'text': entity1, 'type': entity_types[0]},\n",
    "            'entity2': {'text': entity2, 'type': entity_types[1]},\n",
    "            'contexts': [],\n",
    "            'relationship_types': set(),\n",
    "            'confidence': 0.0\n",
    "        }\n",
    "        \n",
    "        # Analyze each relevant sentence\n",
    "        for sent in relevant_sents:\n",
    "            context = self._extract_relationship_context(sent, entity1, entity2)\n",
    "            if context:\n",
    "                relationship['contexts'].append(context)\n",
    "                relationship['relationship_types'].update(context['relationship_types'])\n",
    "                relationship['confidence'] = max(relationship['confidence'], \n",
    "                                              context['confidence'])\n",
    "        \n",
    "        if relationship['contexts']:\n",
    "            relationship['relationship_types'] = list(relationship['relationship_types'])\n",
    "            return relationship\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _extract_relationship_context(self, sent: Span, entity1: str, entity2: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract detailed context for a relationship from a sentence.\n",
    "        \"\"\"\n",
    "        matches = self.matcher(sent)\n",
    "        relationship_types = set()\n",
    "        \n",
    "        # Analyze matches to determine relationship type\n",
    "        for match_id, start, end in matches:\n",
    "            rel_type = sent.vocab.strings[match_id]\n",
    "            relationship_types.add(rel_type)\n",
    "        \n",
    "        # Calculate confidence based on clarity of relationship\n",
    "        confidence = 0.5  # Base confidence\n",
    "        if relationship_types:\n",
    "            confidence += 0.3\n",
    "        if len(sent) < 30:  # Shorter sentences often indicate clearer relationships\n",
    "            confidence += 0.2\n",
    "        \n",
    "        return {\n",
    "            'sentence': sent.text,\n",
    "            'relationship_types': relationship_types,\n",
    "            'confidence': min(1.0, confidence)\n",
    "        }\n",
    "    \n",
    "    def _incorporate_metadata(self, relationships: List[Dict], metadata: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Enhance relationships with temporal and document context from metadata.\n",
    "        \"\"\"\n",
    "        enhanced_relationships = []\n",
    "        \n",
    "        for rel in relationships:\n",
    "            # Add temporal context if available\n",
    "            if 'mentioned_dates' in metadata:\n",
    "                rel['temporal_context'] = metadata['mentioned_dates']\n",
    "            \n",
    "            # Add document metadata\n",
    "            rel['document_context'] = {\n",
    "                'doc_id': metadata.get('doc_id'),\n",
    "                'timestamp': metadata.get('timestamp'),\n",
    "                'document_type': metadata.get('document_type', 'unknown')\n",
    "            }\n",
    "            \n",
    "            enhanced_relationships.append(rel)\n",
    "        \n",
    "        return enhanced_relationships\n",
    "    \n",
    "    def _calculate_confidence_scores(self, entities: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate overall confidence scores for entity detection.\n",
    "        \"\"\"\n",
    "        confidence_scores = {}\n",
    "        \n",
    "        for entity_type, data in entities.items():\n",
    "            avg_confidence = sum(data['confidence_scores'].values()) / len(data['confidence_scores'])\n",
    "            confidence_scores[entity_type] = avg_confidence\n",
    "        \n",
    "        return confidence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(input_file: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process documents with enhanced entity recognition and relationship extraction.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the CSV file from Step 1\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - DataFrame with enhanced entity information\n",
    "        - DataFrame with relationship information\n",
    "    \"\"\"\n",
    "    print(\"Starting enhanced entity recognition and relationship extraction...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = AdvancedEntityAnalyzer()\n",
    "    \n",
    "    # Read input data\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Loaded {len(df)} documents from {input_file}\")\n",
    "    \n",
    "    # Process documents\n",
    "    entities_data = []\n",
    "    relationships_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Convert existing entities to sets\n",
    "            existing_entities = {\n",
    "                'LOC': set(row['LOC'].split('; ')) if pd.notna(row['LOC']) else set(),\n",
    "                'PER': set(row['PER'].split('; ')) if pd.notna(row['PER']) else set(),\n",
    "                'ORG': set(row['ORG'].split('; ')) if pd.notna(row['ORG']) else set(),\n",
    "                'MISC': set(row['MISC'].split('; ')) if pd.notna(row['MISC']) else set()\n",
    "            }\n",
    "            \n",
    "            # Process document\n",
    "            result = analyzer.process_document(\n",
    "                original_text=row['text'],\n",
    "                processed_text=row['processed_text'],\n",
    "                existing_entities=existing_entities,\n",
    "                metadata=eval(row['metadata']) if pd.notna(row['metadata']) else {}\n",
    "            )\n",
    "            \n",
    "            # Store entity information\n",
    "            entities_data.append({\n",
    "                'filename': row['filename'],\n",
    "                'entities': result['entities'],\n",
    "                'confidence_scores': result['confidence_scores']\n",
    "            })\n",
    "            \n",
    "            # Store relationship information\n",
    "            for rel in result['relationships']:\n",
    "                rel['filename'] = row['filename']\n",
    "                relationships_data.append(rel)\n",
    "            \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Processed {idx + 1} documents...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {idx}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create output DataFrames\n",
    "    entities_df = pd.DataFrame(entities_data)\n",
    "    relationships_df = pd.DataFrame(relationships_data)\n",
    "    \n",
    "    # Print processing statistics\n",
    "    processing_time = datetime.now() - start_time\n",
    "    print(f\"\\nProcessing completed in {processing_time}\")\n",
    "    print(f\"Found {len(relationships_df)} relationships across {len(entities_df)} documents\")\n",
    "    \n",
    "    return entities_df, relationships_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhanced entity recognition and relationship extraction...\n",
      "Processing documents from /Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/Dataset/Cleaned_data 29 Jan/process.csv...\n",
      "Starting enhanced entity recognition and relationship extraction...\n",
      "Loaded 1518 documents from /Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/Dataset/Cleaned_data 29 Jan/process.csv\n",
      "Processed 100 documents...\n",
      "Processed 200 documents...\n",
      "Processed 300 documents...\n",
      "Processed 400 documents...\n",
      "Processed 500 documents...\n",
      "Processed 600 documents...\n",
      "Processed 700 documents...\n",
      "Processed 800 documents...\n",
      "Processed 900 documents...\n",
      "Processed 1000 documents...\n",
      "Processed 1100 documents...\n",
      "Processed 1200 documents...\n",
      "Processed 1300 documents...\n",
      "Processed 1400 documents...\n",
      "Processed 1500 documents...\n",
      "\n",
      "Processing completed in 0:00:32.566390\n",
      "Found 11796 relationships across 1518 documents\n",
      "\n",
      "Saving enhanced entity information to /Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/Dataset/Cleaned_data 29 Jan/enhanced_entities.csv\n",
      "Saving relationship information to /Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/Dataset/Cleaned_data 29 Jan/entity_relationships.csv\n",
      "\n",
      "Calculating detailed statistics...\n",
      "\n",
      "=== Processing Statistics ===\n",
      "Total documents processed: 1518\n",
      "\n",
      "--- Entity Statistics ---\n",
      "Total entities found: 19469\n",
      "\n",
      "Entities by type:\n",
      "ORG: 3892 entities (Avg. confidence: 0.98)\n",
      "MISC: 8773 entities (Avg. confidence: 0.93)\n",
      "PER: 3163 entities (Avg. confidence: 0.99)\n",
      "LOC: 3641 entities (Avg. confidence: 0.98)\n",
      "\n",
      "Top 5 most frequent entities by type:\n",
      "\n",
      "ORG:\n",
      "  - Reuters: 64 occurrences\n",
      "  - AI: 35 occurrences\n",
      "  - EU: 28 occurrences\n",
      "  - Microsoft: 27 occurrences\n",
      "  - Google: 25 occurrences\n",
      "\n",
      "MISC:\n",
      "  - two: 210 occurrences\n",
      "  - first: 206 occurrences\n",
      "  - one: 144 occurrences\n",
      "  - Thursday: 127 occurrences\n",
      "  - Tuesday: 117 occurrences\n",
      "\n",
      "PER:\n",
      "  - Lee: 20 occurrences\n",
      "  - Biden: 18 occurrences\n",
      "  - Tan: 16 occurrences\n",
      "  - Joe Biden: 15 occurrences\n",
      "  - COVID-19: 14 occurrences\n",
      "\n",
      "LOC:\n",
      "  - Singapore: 242 occurrences\n",
      "  - China: 178 occurrences\n",
      "  - US: 119 occurrences\n",
      "  - Russia: 71 occurrences\n",
      "  - the United States: 70 occurrences\n",
      "\n",
      "--- Relationship Statistics ---\n",
      "Total relationships found: 11796\n",
      "\n",
      "Relationship types distribution:\n",
      "  COLLABORATION: 59\n",
      "\n",
      "Entity type pair distribution:\n",
      "  PER-ORG: 3289\n",
      "  ORG-PER: 3289\n",
      "  ORG-ORG: 3132\n",
      "  PER-PER: 2086\n",
      "\n",
      "Average relationship confidence: 0.56\n",
      "\n",
      "Total processing time: 0:00:33.350476\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def calculate_entity_statistics(df: pd.DataFrame) -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Calculate detailed statistics about entities and their confidence scores.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - Entity counts by type\n",
    "        - Average confidence scores by type\n",
    "        - Most frequent entities by type\n",
    "    \"\"\"\n",
    "    entity_counts = defaultdict(int)\n",
    "    confidence_scores = defaultdict(list)\n",
    "    frequent_entities = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        entities = row['entities']\n",
    "        for entity_type, data in entities.items():\n",
    "            # Count entities\n",
    "            entity_counts[entity_type] += len(data['entities'])\n",
    "            \n",
    "            # Collect confidence scores\n",
    "            confidence_scores[entity_type].extend(data['confidence_scores'].values())\n",
    "            \n",
    "            # Count entity frequencies\n",
    "            for entity in data['entities']:\n",
    "                frequent_entities[entity_type][entity] += 1\n",
    "    \n",
    "    # Calculate average confidence scores\n",
    "    avg_confidence = {\n",
    "        etype: np.mean(scores) if scores else 0\n",
    "        for etype, scores in confidence_scores.items()\n",
    "    }\n",
    "    \n",
    "    # Get top 5 most frequent entities for each type\n",
    "    top_entities = {\n",
    "        etype: dict(sorted(entities.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "        for etype, entities in frequent_entities.items()\n",
    "    }\n",
    "    \n",
    "    return entity_counts, avg_confidence, top_entities\n",
    "\n",
    "def analyze_relationships(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze relationship patterns and statistics.\n",
    "    \"\"\"\n",
    "    relationship_stats = {\n",
    "        'total_count': len(df),\n",
    "        'type_distribution': defaultdict(int),\n",
    "        'confidence_distribution': [],\n",
    "        'entity_type_pairs': defaultdict(int)\n",
    "    }\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Count relationship types\n",
    "        for rel_type in row['relationship_types']:\n",
    "            relationship_stats['type_distribution'][rel_type] += 1\n",
    "        \n",
    "        # Record confidence scores\n",
    "        relationship_stats['confidence_distribution'].append(row['confidence'])\n",
    "        \n",
    "        # Count entity type pairs\n",
    "        pair_key = f\"{row['entity1']['type']}-{row['entity2']['type']}\"\n",
    "        relationship_stats['entity_type_pairs'][pair_key] += 1\n",
    "    \n",
    "    return relationship_stats\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Starting enhanced entity recognition and relationship extraction...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Input and output files\n",
    "        input_file = '/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/Dataset/Cleaned_data 29 Jan/process.csv'\n",
    "        output_entities = '/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/Dataset/Cleaned_data 29 Jan/enhanced_entities.csv'\n",
    "        output_relationships = '/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/Dataset/Cleaned_data 29 Jan/entity_relationships.csv'\n",
    "        \n",
    "        # Process documents\n",
    "        print(f\"Processing documents from {input_file}...\")\n",
    "        entities_df, relationships_df = process_documents(input_file)\n",
    "        \n",
    "        # Save results\n",
    "        print(f\"\\nSaving enhanced entity information to {output_entities}\")\n",
    "        entities_df.to_csv(output_entities, index=False)\n",
    "        \n",
    "        print(f\"Saving relationship information to {output_relationships}\")\n",
    "        relationships_df.to_csv(output_relationships, index=False)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        print(\"\\nCalculating detailed statistics...\")\n",
    "        entity_counts, confidence_scores, frequent_entities = calculate_entity_statistics(entities_df)\n",
    "        relationship_stats = analyze_relationships(relationships_df)\n",
    "        \n",
    "        # Print comprehensive statistics\n",
    "        print(\"\\n=== Processing Statistics ===\")\n",
    "        print(f\"Total documents processed: {len(entities_df)}\")\n",
    "        \n",
    "        print(\"\\n--- Entity Statistics ---\")\n",
    "        print(f\"Total entities found: {sum(entity_counts.values())}\")\n",
    "        print(\"\\nEntities by type:\")\n",
    "        for entity_type, count in entity_counts.items():\n",
    "            print(f\"{entity_type}: {count} entities (Avg. confidence: {confidence_scores[entity_type]:.2f})\")\n",
    "        \n",
    "        print(\"\\nTop 5 most frequent entities by type:\")\n",
    "        for entity_type, entities in frequent_entities.items():\n",
    "            print(f\"\\n{entity_type}:\")\n",
    "            for entity, count in entities.items():\n",
    "                print(f\"  - {entity}: {count} occurrences\")\n",
    "        \n",
    "        print(\"\\n--- Relationship Statistics ---\")\n",
    "        print(f\"Total relationships found: {relationship_stats['total_count']}\")\n",
    "        \n",
    "        print(\"\\nRelationship types distribution:\")\n",
    "        for rel_type, count in relationship_stats['type_distribution'].items():\n",
    "            print(f\"  {rel_type}: {count}\")\n",
    "        \n",
    "        print(\"\\nEntity type pair distribution:\")\n",
    "        for pair, count in relationship_stats['entity_type_pairs'].items():\n",
    "            print(f\"  {pair}: {count}\")\n",
    "        \n",
    "        avg_confidence = np.mean(relationship_stats['confidence_distribution'])\n",
    "        print(f\"\\nAverage relationship confidence: {avg_confidence:.2f}\")\n",
    "        \n",
    "        # Save statistics to file\n",
    "        statistics = {\n",
    "            'entity_statistics': {\n",
    "                'counts': entity_counts,\n",
    "                'confidence_scores': confidence_scores,\n",
    "                'frequent_entities': frequent_entities\n",
    "            },\n",
    "            'relationship_statistics': relationship_stats,\n",
    "            'processing_info': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'processing_time': str(datetime.now() - start_time),\n",
    "                'document_count': len(entities_df)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open('processing_statistics.json', 'w') as f:\n",
    "            json.dump(statistics, f, indent=2)\n",
    "        \n",
    "        # Print processing time\n",
    "        processing_time = datetime.now() - start_time\n",
    "        print(f\"\\nTotal processing time: {processing_time}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\", file=sys.stderr)\n",
    "        print(\"\\nFull error details:\", file=sys.stderr)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smubia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
