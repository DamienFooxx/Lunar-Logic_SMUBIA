{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCrimeProcessor:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_name = \"Luna-Skywalker/BERT-crime-analysis\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name).to(self.device)\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.confidence_threshold = 0.7\n",
    "\n",
    "    def process_text(self, text: str) -> Dict[str, float]:\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)[0]\n",
    "            \n",
    "        predictions = []\n",
    "        for idx, prob in enumerate(probs):\n",
    "            if prob > self.confidence_threshold:\n",
    "                label = self.model.config.id2label[idx]\n",
    "                predictions.append((label, float(prob)))\n",
    "        \n",
    "        return sorted(predictions, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.crime_processor = BERTCrimeProcessor()\n",
    "        self.nlp = self.crime_processor.nlp\n",
    "        \n",
    "    def process_document(self, text: str, doc_id: str) -> Dict:\n",
    "        try:\n",
    "            # Clean and process text\n",
    "            cleaned_text = self.clean_text(text)\n",
    "            \n",
    "            # Get BERT crime predictions\n",
    "            crime_predictions = self.crime_processor.process_text(cleaned_text)\n",
    "            \n",
    "            # Extract entities\n",
    "            entities = self.extract_entities(cleaned_text)\n",
    "            \n",
    "            # Create result\n",
    "            result = {\n",
    "                'filename': doc_id,\n",
    "                'text': text,\n",
    "                'processed_text': cleaned_text,\n",
    "                'LOC': '; '.join(entities.get('LOC', [])),\n",
    "                'PER': '; '.join(entities.get('PER', [])),\n",
    "                'ORG': '; '.join(entities.get('ORG', [])),\n",
    "                'MISC': '; '.join(entities.get('MISC', [])),\n",
    "                'CRIME_TYPES': '; '.join([crime for crime, _ in crime_predictions]),\n",
    "                'CRIME_CONFIDENCE': '; '.join([f\"{conf:.3f}\" for _, conf in crime_predictions]),\n",
    "                'metadata': self.extract_metadata(cleaned_text, doc_id)\n",
    "            }\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing document {doc_id}: {str(e)}\")\n",
    "            return self.create_empty_result(doc_id, text)\n",
    "\n",
    "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
    "        doc = self.nlp(text)\n",
    "        entities = {\n",
    "            'LOC': set(),\n",
    "            'PER': set(),\n",
    "            'ORG': set(),\n",
    "            'MISC': set()\n",
    "        }\n",
    "        \n",
    "        label_mapping = {\n",
    "            'GPE': 'LOC', 'LOC': 'LOC', 'FAC': 'LOC',\n",
    "            'PERSON': 'PER',\n",
    "            'ORG': 'ORG',\n",
    "            'PRODUCT': 'MISC', 'EVENT': 'MISC', 'WORK_OF_ART': 'MISC',\n",
    "            'LAW': 'MISC', 'LANGUAGE': 'MISC', 'NORP': 'MISC'\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            category = label_mapping.get(ent.label_, 'MISC')\n",
    "            entities[category].add(ent.text)\n",
    "        \n",
    "        return {k: sorted(v) for k, v in entities.items()}\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).strip()\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "    def create_empty_result(self, doc_id: str, text: str) -> Dict:\n",
    "        return {\n",
    "            'filename': doc_id,\n",
    "            'text': text,\n",
    "            'processed_text': '',\n",
    "            'LOC': '',\n",
    "            'PER': '',\n",
    "            'ORG': '',\n",
    "            'MISC': '',\n",
    "            'CRIME_TYPES': '',\n",
    "            'CRIME_CONFIDENCE': '',\n",
    "            'metadata': {'error': 'Processing failed'}\n",
    "        }\n",
    "\n",
    "    def extract_metadata(self, text: str, doc_id: str) -> Dict:\n",
    "        doc = self.nlp(text)\n",
    "        dates = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "        \n",
    "        return {\n",
    "            'doc_id': doc_id,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'length': len(text),\n",
    "            'dates': dates\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(df: pd.DataFrame, batch_size: int = 32) -> pd.DataFrame:\n",
    "    preprocessor = EnhancedTextPreprocessor()\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i + batch_size]\n",
    "        for _, row in batch.iterrows():\n",
    "            text = row.get( 'Extracted Text', row.get('Text', ''))\n",
    "            filename = row.get('Filename', row.get('PDF Path', f'doc_{len(results)}'))\n",
    "            result = preprocessor.process_document(text, filename)\n",
    "            results.append(result)\n",
    "        \n",
    "        if (i + batch_size) % 100 == 0:\n",
    "            logging.info(f\"Processed {i + batch_size}/{len(df)} documents\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = '/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/FINAL FINAL PLEASE/Data/pdfs.csv'\n",
    "news_path = '/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/FINAL FINAL PLEASE/Data/news.csv'\n",
    "output_path = '/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/FINAL FINAL PLEASE/Data/process1_3.csv'\n",
    "\n",
    "pdf_df = pd.read_csv(pdf_path)\n",
    "news_df = pd.read_csv(news_path)\n",
    "input_df = pd.concat([news_df, pdf_df], ignore_index=True)\n",
    "\n",
    "processed_df = process_dataset(input_df)\n",
    "processed_df.to_csv(output_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smubia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
