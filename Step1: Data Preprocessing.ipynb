{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below outlines the class that will give us the TextPreprocessor to help us clean the data.\n",
    "Let me explain the preprocessing pipeline I've created in simple terms:\n",
    "\n",
    "First, we created a TextPreprocessor class that handles all the cleaning and processing of our documents. \n",
    "The main components of our preprocessing pipeline are:\n",
    "a. Text Cleaning (clean_text method):\n",
    "\n",
    "Removes extra spaces and standardizes how lines break\n",
    "Makes sure all quotes are in the same format\n",
    "Removes strange characters while keeping important ones like periods and commas\n",
    "Makes sure spacing around punctuation is consistent\n",
    "\n",
    "b. Metadata Extraction (extract_metadata method):\n",
    "\n",
    "Collects important information about each document\n",
    "Records when we processed it\n",
    "Figures out how long the document is\n",
    "Identifies what language it's in\n",
    "Determines what kind of document it is (like an investigation report or memo)\n",
    "Finds any dates mentioned in the text\n",
    "\n",
    "c. Document Segmentation (segment_document method):\n",
    "\n",
    "Breaks the document into logical pieces\n",
    "Identifies different parts like \"allegation,\" \"background,\" and \"investigation\"\n",
    "This helps us later when we want to find specific types of information\n",
    "\n",
    "\n",
    "The process_dataset function ties everything together:\n",
    "\n",
    "Takes our spreadsheet of documents\n",
    "Runs each document through the preprocessing pipeline\n",
    "Creates a new, enhanced version of our data with all the processed information\n",
    "\n",
    "\n",
    "What makes this preprocessing special for our criminal investigation data:\n",
    "\n",
    "It's designed to handle investigative documents specifically, recognizing parts like \"allegations\" and \"background information\"\n",
    "It preserves important elements like dates, monetary values, and company names\n",
    "It creates a structured format that will make it easier to extract relationships between entities later\n",
    "It maintains a clear record of how we processed each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Enhanced version of TextPreprocessor with crime type detection and structured entity extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load the English language model from spaCy\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "        \n",
    "        # Initialize storage for document metadata\n",
    "        self.document_metadata = {}\n",
    "        \n",
    "        # Define crime types and their related keywords\n",
    "        self.crime_patterns = {\n",
    "            'FINANCIAL_CRIME': [\n",
    "                'fraud', 'embezzlement', 'money laundering', 'bribery', 'corruption',\n",
    "                'insider trading', 'tax evasion', 'ponzi scheme', 'financial crime',\n",
    "                'misappropriation', 'illegal transaction'\n",
    "            ],\n",
    "            'CYBERCRIME': [\n",
    "                'hacking', 'cyber attack', 'data breach', 'ransomware', 'phishing',\n",
    "                'malware', 'identity theft', 'cyber crime', 'cyber security breach',\n",
    "                'computer fraud', 'network intrusion'\n",
    "            ],\n",
    "            'VIOLENT_CRIME': [\n",
    "                'assault', 'murder', 'homicide', 'robbery', 'kidnapping',\n",
    "                'terrorism', 'shooting', 'violent crime', 'armed robbery',\n",
    "                'physical assault', 'violent attack'\n",
    "            ],\n",
    "            'ORGANIZED_CRIME': [\n",
    "                'trafficking', 'smuggling', 'cartel', 'syndicate', 'gang',\n",
    "                'organized crime', 'criminal enterprise', 'criminal network',\n",
    "                'criminal organization', 'illegal operation'\n",
    "            ],\n",
    "            'PROPERTY_CRIME': [\n",
    "                'theft', 'burglary', 'vandalism', 'arson', 'shoplifting',\n",
    "                'property damage', 'stolen property', 'breaking and entering',\n",
    "                'property crime', 'larceny'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Compile regex patterns for crime detection\n",
    "        self.crime_regex = self._compile_crime_patterns()\n",
    "        \n",
    "    def _compile_crime_patterns(self) -> Dict[str, re.Pattern]:\n",
    "        \"\"\"\n",
    "        Compile regex patterns for each crime type.\n",
    "        Returns dictionary of compiled regex patterns.\n",
    "        \"\"\"\n",
    "        compiled_patterns = {}\n",
    "        for crime_type, keywords in self.crime_patterns.items():\n",
    "            # Create regex pattern that matches any keyword, case insensitive\n",
    "            pattern = r'\\b(' + '|'.join(re.escape(keyword) for keyword in keywords) + r')\\b'\n",
    "            compiled_patterns[crime_type] = re.compile(pattern, re.IGNORECASE)\n",
    "        return compiled_patterns\n",
    "    \n",
    "    def detect_crime_types(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Detect crime types mentioned in the text.\n",
    "        Returns list of detected crime types.\n",
    "        \"\"\"\n",
    "        detected_crimes = set()\n",
    "        for crime_type, pattern in self.crime_regex.items():\n",
    "            if pattern.search(text):\n",
    "                detected_crimes.add(crime_type)\n",
    "        return list(detected_crimes)\n",
    "    \n",
    "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Extract entities by type (LOC, PER, MISC, ORG).\n",
    "        Returns dictionary of entity lists by type.\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        entities = defaultdict(set)\n",
    "        \n",
    "        # Map spaCy entity labels to our desired categories\n",
    "        label_mapping = {\n",
    "            'GPE': 'LOC',\n",
    "            'LOC': 'LOC',\n",
    "            'PERSON': 'PER',\n",
    "            'ORG': 'ORG',\n",
    "            'FAC': 'LOC',  # Facilities are mapped to locations\n",
    "            'NORP': 'MISC',  # Nationalities, religious or political groups\n",
    "            'PRODUCT': 'MISC',\n",
    "            'EVENT': 'MISC',\n",
    "            'WORK_OF_ART': 'MISC',\n",
    "            'LAW': 'MISC',\n",
    "            'LANGUAGE': 'MISC'\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            # Map the entity to our categories\n",
    "            category = label_mapping.get(ent.label_, 'MISC')\n",
    "            entities[category].add(ent.text)\n",
    "        \n",
    "        # Convert sets to sorted lists\n",
    "        return {k: sorted(v) for k, v in entities.items()}\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and standardize text content.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "            \n",
    "        # Convert to string if not already\n",
    "        text = str(text)\n",
    "        \n",
    "        # Remove extra whitespace and standardize newlines\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Standardize quotes\n",
    "        text = text.replace('\"', '\"').replace('\"', '\"')\n",
    "        \n",
    "        # Remove control characters\n",
    "        text = ''.join(char for char in text if ord(char) >= 32 or char == '\\n')\n",
    "        \n",
    "        # Standardize spacing around punctuation\n",
    "        text = re.sub(r'\\s*([.,!?;:])\\s*', r'\\1 ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def process_document(self, text: str, doc_id: str) -> Tuple[Dict[str, List[str]], List[str], Dict]:\n",
    "        \"\"\"\n",
    "        Process a single document to extract entities and crime types.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw text content\n",
    "            doc_id (str): Document identifier\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[Dict[str, List[str]], List[str], Dict]: \n",
    "            - Extracted entities by category\n",
    "            - Detected crime types\n",
    "            - Document metadata\n",
    "        \"\"\"\n",
    "        # Clean the text first\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = self.extract_entities(cleaned_text)\n",
    "        \n",
    "        # Detect crime types\n",
    "        crime_types = self.detect_crime_types(cleaned_text)\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = self.extract_metadata(cleaned_text, doc_id)\n",
    "        \n",
    "        return entities, crime_types, metadata\n",
    "    \n",
    "    def extract_metadata(self, text: str, doc_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract metadata from the document text.\n",
    "        \"\"\"\n",
    "        metadata = {\n",
    "            'doc_id': doc_id,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'length': len(text)\n",
    "        }\n",
    "        \n",
    "        # Extract dates mentioned in the text\n",
    "        doc = self.nlp(text)\n",
    "        dates = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "        if dates:\n",
    "            metadata['mentioned_dates'] = dates\n",
    "            \n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the entire dataset using the EnhancedTextPreprocessor.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with document text\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame in desired format\n",
    "    \"\"\"\n",
    "    # Initialize the preprocessor\n",
    "    preprocessor = EnhancedTextPreprocessor()\n",
    "    \n",
    "    # Lists to store processed data\n",
    "    all_data = []\n",
    "    \n",
    "    # Process each document\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Extract text and filename\n",
    "            text = row['Extracted Text']\n",
    "            filename = row['Filename']\n",
    "            \n",
    "            # Process the document\n",
    "            entities, crime_types, metadata = preprocessor.process_document(text, filename)\n",
    "            \n",
    "            # Prepare row data\n",
    "            row_data = {\n",
    "                'filename': filename,\n",
    "                'LOC': '; '.join(entities.get('LOC', [])),\n",
    "                'PER': '; '.join(entities.get('PER', [])),\n",
    "                'ORG': '; '.join(entities.get('ORG', [])),\n",
    "                'MISC': '; '.join(entities.get('MISC', [])),\n",
    "                'CRIME_TYPES': '; '.join(crime_types)\n",
    "            }\n",
    "            \n",
    "            all_data.append(row_data)\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Processed {idx + 1} documents...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {idx}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    output_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing...\n",
      "Reading datasets...\n",
      "Read 1509 news excerpts and 9 wikileaks documents\n",
      "Combining datasets...\n",
      "\n",
      "Processing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x107a55c90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/smubia/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 documents...\n",
      "Processed 200 documents...\n",
      "Processed 300 documents...\n",
      "Processed 400 documents...\n",
      "Processed 500 documents...\n",
      "Processed 600 documents...\n",
      "Processed 700 documents...\n"
     ]
    }
   ],
   "source": [
    "# Implementation\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Starting data processing...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Read the datasets\n",
    "        print(\"Reading datasets...\")\n",
    "        news_df = pd.read_csv('/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/Dataset/Unstructured csvs/news_excerpts_parsed.xlsx - Sheet1 (1).csv')\n",
    "        wikileaks_df = pd.read_csv('/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/Dataset/Unstructured csvs/wikileaks_parsed.xlsx - Sheet1 copy.csv')\n",
    "        \n",
    "        print(f\"Read {len(news_df)} news excerpts and {len(wikileaks_df)} wikileaks documents\")\n",
    "        \n",
    "        # Rename columns in wikileaks_df to match news_df\n",
    "        wikileaks_df = wikileaks_df.rename(columns={\n",
    "            'PDF Path': 'Filename',\n",
    "            'Text': 'Extracted Text'\n",
    "        })\n",
    "        \n",
    "        # Combine datasets\n",
    "        print(\"Combining datasets...\")\n",
    "        combined_df = pd.concat([news_df, wikileaks_df], ignore_index=True)\n",
    "        \n",
    "        # Process the combined dataset\n",
    "        print(\"\\nProcessing documents...\")\n",
    "        processed_df = process_dataset(combined_df)\n",
    "        \n",
    "        # Save the processed data\n",
    "        output_path = '/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/Dataset/Cleaned_data 29 Jan/process.csv'\n",
    "        print(f\"\\nSaving processed data to {output_path}\")\n",
    "        processed_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nProcessing Statistics:\")\n",
    "        print(f\"Total documents processed: {len(processed_df)}\")\n",
    "        \n",
    "        print(\"\\nCrime type distribution:\")\n",
    "        crime_types = processed_df['CRIME_TYPES'].str.split('; ').explode()\n",
    "        print(crime_types.value_counts().head())\n",
    "        \n",
    "        print(\"\\nEntity statistics:\")\n",
    "        for col in ['LOC', 'PER', 'ORG']:\n",
    "            entities = processed_df[col].str.split('; ').explode()\n",
    "            print(f\"\\nTop 5 most mentioned {col}:\")\n",
    "            print(entities.value_counts().head())\n",
    "        \n",
    "        # Calculate and print processing time\n",
    "        processing_time = datetime.now() - start_time\n",
    "        print(f\"\\nTotal processing time: {processing_time}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smubia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
