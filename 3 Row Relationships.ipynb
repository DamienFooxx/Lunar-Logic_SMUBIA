{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert/albert-large-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlbertForMaskedLM(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertSdpaAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (ffn_output): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predictions): AlbertMLMHead(\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (decoder): Linear(in_features=128, out_features=30000, bias=True)\n",
       "    (activation): NewGELUActivation()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-large-v2\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"albert/albert-large-v2\")\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrganizationalCrimeExtractor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "        \n",
    "        # Organization-Person patterns\n",
    "        self.org_person_patterns = {\n",
    "            'INSTRUCTED': r'(?i)(instruct|direct|order|command|ask)',\n",
    "            'COLLABORATED': r'(?i)(collaborate|work with|partner|conspire)',\n",
    "            'FUNDED': r'(?i)(fund|finance|pay|sponsor)',\n",
    "            'EMPLOYED': r'(?i)(employ|hire|contract)',\n",
    "            'FACILITATED': r'(?i)(facilitate|enable|help|assist)',\n",
    "            'SUPERVISED': r'(?i)(supervise|oversee|manage)',\n",
    "            'AUTHORIZED': r'(?i)(authorize|approve|permit|allow)'\n",
    "        }\n",
    "        \n",
    "        # Organization-Organization patterns\n",
    "        self.org_org_patterns = {\n",
    "            'PARTNERSHIP': r'(?i)(partner|collaborate|alliance|joint venture)',\n",
    "            'FUNDING': r'(?i)(fund|invest|finance|transfer money)',\n",
    "            'CONTROL': r'(?i)(control|own|acquire|merge|takeover)',\n",
    "            'SUPPLY': r'(?i)(supply|provide|deliver|distribute)',\n",
    "            'CONSPIRACY': r'(?i)(conspire|scheme|plot|collude)',\n",
    "            'FRONT': r'(?i)(front|shell|cover|facade)',\n",
    "            'FACILITATION': r'(?i)(facilitate|enable|support|assist)'\n",
    "        }\n",
    "        \n",
    "        # Criminal activity indicators\n",
    "        self.crime_indicators = {\n",
    "            'FRAUD': r'(?i)(fraud|scam|deceive|misrepresent)',\n",
    "            'MONEY_LAUNDERING': r'(?i)(launder|clean money|illegal funds)',\n",
    "            'CORRUPTION': r'(?i)(corrupt|bribe|kickback)',\n",
    "            'TRAFFICKING': r'(?i)(traffic|smuggle|illegal trade)',\n",
    "            'CYBERCRIME': r'(?i)(hack|cyber|digital theft|ransomware)',\n",
    "            'CONSPIRACY': r'(?i)(conspire|plot|scheme|plan)',\n",
    "            'TAX_EVASION': r'(?i)(tax evasion|tax fraud|undeclared)',\n",
    "            'ILLEGAL_TRADE': r'(?i)(illegal trade|black market|contraband)'\n",
    "        }\n",
    "\n",
    "    def extract_relationships(self, row: pd.Series) -> List[Dict]:\n",
    "        relationships = []\n",
    "        text = row['processed_text']\n",
    "        \n",
    "        if pd.isna(text) or text.strip() == '':\n",
    "            return relationships\n",
    "\n",
    "        doc = self.nlp(text)\n",
    "        entities = self._parse_entities(row)\n",
    "        \n",
    "        # Extract org-person relationships\n",
    "        relationships.extend(self._extract_org_person_relations(doc, entities))\n",
    "        # Extract org-org relationships\n",
    "        relationships.extend(self._extract_org_org_relations(doc, entities))\n",
    "        # Extract criminal activities\n",
    "        relationships.extend(self._extract_criminal_activities(doc, entities))\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "    def _parse_entities(self, row: pd.Series) -> Dict[str, set]:\n",
    "        entities = {}\n",
    "        for field in ['PER', 'ORG', 'CRIME_TYPES']:\n",
    "            if pd.notna(row[field]) and row[field].strip():\n",
    "                entities[field] = {\n",
    "                    self._clean_entity_name(e.strip()) \n",
    "                    for e in row[field].split(';') \n",
    "                    if e.strip()\n",
    "                }\n",
    "            else:\n",
    "                entities[field] = set()\n",
    "        return entities\n",
    "\n",
    "    def _clean_entity_name(self, name: str) -> str:\n",
    "        prefixes = ['mr.', 'mrs.', 'ms.', 'dr.', 'the']\n",
    "        name = name.lower()\n",
    "        for prefix in prefixes:\n",
    "            if name.startswith(prefix + ' '):\n",
    "                name = name[len(prefix)+1:]\n",
    "        return name.strip().title()\n",
    "\n",
    "    def _extract_org_person_relations(self, doc, entities: Dict[str, set]) -> List[Dict]:\n",
    "        relationships = []\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            sent_text = sent.text.lower()\n",
    "            \n",
    "            for org in entities['ORG']:\n",
    "                for person in entities['PER']:\n",
    "                    if org.lower() in sent_text and person.lower() in sent_text:\n",
    "                        for rel_type, pattern in self.org_person_patterns.items():\n",
    "                            if re.search(pattern, sent_text):\n",
    "                                crime_types = [ct for ct in entities['CRIME_TYPES'] \n",
    "                                            if ct.lower() in sent_text]\n",
    "                                crime_type = crime_types[0] if crime_types else \"Unknown\"\n",
    "                                \n",
    "                                relationships.append({\n",
    "                                    'subject': org,\n",
    "                                    'subject_type': 'ORG',\n",
    "                                    'predicate': f'ORG_PER_{rel_type}',\n",
    "                                    'object': person,\n",
    "                                    'object_type': 'PER',\n",
    "                                    'crime_type': crime_type,\n",
    "                                    'sentence': sent.text,\n",
    "                                    'evidence_strength': self._assess_evidence_strength(sent_text)\n",
    "                                })\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "    def _extract_org_org_relations(self, doc, entities: Dict[str, set]) -> List[Dict]:\n",
    "        \"\"\"Extract relationships between organizations.\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            sent_text = sent.text.lower()\n",
    "            \n",
    "            # Check pairs of organizations\n",
    "            orgs = list(entities['ORG'])\n",
    "            for i, org1 in enumerate(orgs):\n",
    "                for org2 in orgs[i+1:]:  # Avoid self-relationships\n",
    "                    if org1.lower() in sent_text and org2.lower() in sent_text:\n",
    "                        for rel_type, pattern in self.org_org_patterns.items():\n",
    "                            if re.search(pattern, sent_text):\n",
    "                                crime_types = [ct for ct in entities['CRIME_TYPES'] \n",
    "                                            if ct.lower() in sent_text]\n",
    "                                crime_type = crime_types[0] if crime_types else \"Unknown\"\n",
    "                                \n",
    "                                relationships.append({\n",
    "                                    'subject': org1,\n",
    "                                    'subject_type': 'ORG',\n",
    "                                    'predicate': f'ORG_ORG_{rel_type}',\n",
    "                                    'object': org2,\n",
    "                                    'object_type': 'ORG',\n",
    "                                    'crime_type': crime_type,\n",
    "                                    'sentence': sent.text,\n",
    "                                    'evidence_strength': self._assess_evidence_strength(sent_text)\n",
    "                                })\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "    def _extract_criminal_activities(self, doc, entities: Dict[str, set]) -> List[Dict]:\n",
    "        relationships = []\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            sent_text = sent.text.lower()\n",
    "            \n",
    "            for crime_type, pattern in self.crime_indicators.items():\n",
    "                if re.search(pattern, sent_text):\n",
    "                    # Find involved organizations and persons\n",
    "                    orgs = [org for org in entities['ORG'] \n",
    "                           if org.lower() in sent_text]\n",
    "                    persons = [per for per in entities['PER'] \n",
    "                             if per.lower() in sent_text]\n",
    "                    \n",
    "                    # Create org-person relationships\n",
    "                    for org in orgs:\n",
    "                        for person in persons:\n",
    "                            relationships.append({\n",
    "                                'subject': org,\n",
    "                                'subject_type': 'ORG',\n",
    "                                'predicate': crime_type,\n",
    "                                'object': person,\n",
    "                                'object_type': 'PER',\n",
    "                                'crime_type': crime_type,\n",
    "                                'sentence': sent.text,\n",
    "                                'evidence_strength': self._assess_evidence_strength(sent_text)\n",
    "                            })\n",
    "                    \n",
    "                    # Create org-org relationships\n",
    "                    for i, org1 in enumerate(orgs):\n",
    "                        for org2 in orgs[i+1:]:\n",
    "                            relationships.append({\n",
    "                                'subject': org1,\n",
    "                                'subject_type': 'ORG',\n",
    "                                'predicate': crime_type,\n",
    "                                'object': org2,\n",
    "                                'object_type': 'ORG',\n",
    "                                'crime_type': crime_type,\n",
    "                                'sentence': sent.text,\n",
    "                                'evidence_strength': self._assess_evidence_strength(sent_text)\n",
    "                            })\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "    def _assess_evidence_strength(self, text: str) -> str:\n",
    "        strong_indicators = r'(?i)(confirm|prove|evidence|document|record|witness)'\n",
    "        moderate_indicators = r'(?i)(allege|suspect|believe|report)'\n",
    "        weak_indicators = r'(?i)(rumor|might|maybe|possibly|could)'\n",
    "        \n",
    "        if re.search(strong_indicators, text):\n",
    "            return 'strong'\n",
    "        elif re.search(moderate_indicators, text):\n",
    "            return 'moderate'\n",
    "        elif re.search(weak_indicators, text):\n",
    "            return 'weak'\n",
    "        return 'unspecified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    extractor = OrganizationalCrimeExtractor()\n",
    "    relationships = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        row_relationships = extractor.extract_relationships(row)\n",
    "        for rel in row_relationships:\n",
    "            rel['source_idx'] = idx\n",
    "        relationships.extend(row_relationships)\n",
    "    \n",
    "    rel_df = pd.DataFrame(relationships)\n",
    "    \n",
    "    if not rel_df.empty:\n",
    "        print(\"\\nRelationship Analysis:\")\n",
    "        print(f\"Total relationships: {len(rel_df)}\")\n",
    "        print(\"\\nRelationship types:\")\n",
    "        print(rel_df['predicate'].value_counts())\n",
    "        print(\"\\nEntity type pairs:\")\n",
    "        print(pd.crosstab(rel_df['subject_type'], rel_df['object_type']))\n",
    "        print(\"\\nEvidence strength distribution:\")\n",
    "        print(rel_df['evidence_strength'].value_counts())\n",
    "        print(\"\\nTop crime types:\")\n",
    "        print(rel_df['crime_type'].value_counts().head())\n",
    "    \n",
    "    return rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relationship Analysis:\n",
      "Total relationships: 15319\n",
      "\n",
      "Relationship types:\n",
      "predicate\n",
      "ORG_ORG_FUNDING         3579\n",
      "ORG_PER_INSTRUCTED      1762\n",
      "ORG_PER_FUNDED          1705\n",
      "ORG_PER_SUPERVISED      1051\n",
      "ORG_ORG_CONTROL         1038\n",
      "ORG_PER_EMPLOYED        1029\n",
      "ORG_ORG_SUPPLY           947\n",
      "FRAUD                    763\n",
      "CONSPIRACY               687\n",
      "ORG_ORG_FACILITATION     665\n",
      "ORG_PER_AUTHORIZED       407\n",
      "CORRUPTION               372\n",
      "ORG_ORG_PARTNERSHIP      363\n",
      "ORG_PER_FACILITATED      247\n",
      "ORG_ORG_FRONT            244\n",
      "TRAFFICKING              177\n",
      "ORG_PER_COLLABORATED     133\n",
      "ORG_ORG_CONSPIRACY        88\n",
      "MONEY_LAUNDERING          35\n",
      "CYBERCRIME                27\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Entity type pairs:\n",
      "object_type    ORG   PER\n",
      "subject_type            \n",
      "ORG           8088  7231\n",
      "\n",
      "Evidence strength distribution:\n",
      "evidence_strength\n",
      "unspecified    9415\n",
      "strong         2844\n",
      "moderate       2783\n",
      "weak            277\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top crime types:\n",
      "crime_type\n",
      "Unknown        13255\n",
      "FRAUD            763\n",
      "CONSPIRACY       687\n",
      "CORRUPTION       372\n",
      "TRAFFICKING      177\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load and process data\n",
    "    df = pd.read_csv('/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/FINAL FINAL PLEASE/Data/process2_cleaned.csv')\n",
    "    relationships_df = process_dataset(df)\n",
    "    \n",
    "    # Save outputs\n",
    "    relationships_df.to_csv('/Users/damienfoo/Desktop/SMUBIA Datathon Lunar Logic/FINAL FINAL PLEASE/Data/process3_crime_relationships_enhanced.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freshsmubia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
